<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        
        <!-- Title -->
        <title>
            CS499/599 | AI539 :: F23 :: Trustworthy ML
        </title>

        <!-- SEO -->
        <meta name="author" content="Sanghyun Hong">
        <meta name="description" content="Trustworthy Machine Learning Oregon State University">
        <meta name="keywords" content="sanghyun hong albert Trustworthy Machine Learning oregon state university">

        <!-- Favicon -->
        <link rel="shortcut icon" href="/resources/favicon.ico" type="image/x-icon">

        <!-- Prevent js cache by Chrome -->
        <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
        <meta http-equiv="Pragma" content="no-cache" />
        <meta http-equiv="Expires" content="0" />

        <!-- Bootstrap start -->
        <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Arial:300,400,500,700" />
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

        <!-- CSS -->
        <link rel="stylesheet" type="text/css" href="/css/osu-fonts.css">
        <link rel="stylesheet" type="text/css" href="/css/osu-styles.css">

    </head>
    <body>

        <!-- Content -->
        <div class="container">

            <!-- Title -->
            <h2 class="title">
                <b>CS499/579 | AI539 :: F23 :: Trustworthy Machine Learning</b>
            </h2>

            <hr noshade="" size="1">

            <!-- Top navigation bar -->
            <ul class="nav">
                <li class="nav-item">
                    <a class="nav-link" href="index.html"><h5>Home</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="syllabus.html"><h5>Syllabus</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="critiques.html"><h5>Critique, Presentation</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" aria-current="page" href="homework.html"><h5>Homework</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="project.html"><h5>Project</h5></a>
                </li>
                <!-- <li class="nav-item">
                    <a class="nav-link" href="vs_sanghyun.html"><h5>vs. Sanghyun</h5></a>
                </li> -->
            </ul>

            <hr noshade="" size="1">

            
            <!-- Homeworks -->
            <div class="accordion" id="homeworkExample">


                <!-- HW1 -->
                <div class="card">
                    <div class="card-header" id="headingOne">
                        <button class="btn btn-link" 
                            type="button" 
                            style="color: black;"
                            data-toggle="collapse" 
                            data-target="#homework1" 
                            aria-expanded="false" 
                            aria-controls="homework1">
                            <h4>Homework 1: Build Your Own Models</h4>
                        </button>
                    </div>
              
                    <div id="homework1" 
                         class="collapse" 
                         aria-labelledby="headingOne"
                         data-parent="#homeworkExample">
                        <div class="card-body">

                            <!-- :: Deadlines -->
                            <h5>Important Dates</h5>
                            <ul>
                                <li><b style="color: #D73F09;">Check out the syllabus</b></li>
                            </ul>

                            <!-- :: Introduction -->
                            <h5 style="margin-top: 20px;">Homework Overview</h5>
                            <p>
                                The learning objective of this homework is for you to create a codebase to train and evaluate various deep neural network (DNN) models. You also need to analyze the impact of various factors (that you can control during training) on the final DNN models. You will use this codebase and the trained models to complete the homework assignments (HW 2, 3, and 4) throughout the term.
                            </p>


                            <!-- :: Setup -->
                            <h5 style="margin-top: 20px;">Initial Setup</h5>
                            <p>
                                To begin with, you can choose any deep learning framework that you're already familiar with (e.g., <a href="https://pytorch.org" target="_blank">PyTorch</a>, <a href="https://www.tensorflow.org" target="_blank">TensorFlow</a>, or <a href="https://github.com/google/objax" target="_blank">ObJAX</a>). If you are not familiar with any of these frameworks, you can start with PyTorch or TensorFlow (> v2.0). These are popular choices, and you can find many tutorials [<a href="https://pytorch.org/tutorials/index.html" target="_blank">link</a>] or example code [<a href="https://github.com/pytorch/examples" target="_blank">link</a>] from the Internet.<br/><br>

                                <b style="color: #D73F09;">[Note]</b> I do NOT recommend copying and pasting the sample code found from the Internet. It will be an easy solution for this homework. However, later on, you may have difficulty understanding the attacks and defenses. For example, some attacks (and defenses) require you to know how the deep learning framework computes gradients and how you can manipulate (or control) them.

                                <h6>Datasets and DNN Models</h6>

                                We will limit our scope to two popular image classification datasets: MNIST [<a href="https://pytorch.org/vision/stable/datasets.html#mnist" target="_blank">link</a>] and CIFAR-10 [<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">link</a>]. Most deep learning frameworks support those datasets by default. We will also use three DNNs: LeNet [<a href="https://en.wikipedia.org/wiki/LeNet" target="_blank">link</a>], VGG16 [<a href="https://arxiv.org/abs/1409.1556" target="_blank">link</a>] and ResNet18 [<a href="https://arxiv.org/abs/1512.03385" target="_blank">link</a>]. I added the links to the original papers.<br/><br/>

                                <h6>Recommended Code Structure</h6>
                                <code>
                                    Root<br/>
                                    - models: a dir containing your model definitions.<br/>
                                    - reports: a dir where you will include your write-up.<br/>
                                    - datasets.py: a Python script containing functions for loading datasets.<br/>
                                    - train.py: a Python script for training a model.<br/>
                                    - train.sh: a bash-shell script for training multiple models.<br/>
                                    - valid.py: a Python script for evaluating a pre-trained model.<br/>
                                    ...<br/><br/>
                                </code>

                                Note that this is an example code structure; you can find many nice examples from the Internet [<a href="https://cs230.stanford.edu/blog/tips/" target="_blank">example</a>].
                            </p>


                            <!-- :: Task 1 -->
                            <h5 style="margin-top: 20px;">Task I: Train and Evaluate Your Models</h5>
                            
                            The first task is simple; train 6 DNN models. You will train 3 DNNs (LeNet, VGG16, and ResNet18) on 2 datasets (MNIST and CIFAR-10). You need to measure your model's performance with two metrics: classification accuracy and loss. You can compute them on both the training and testing data.<br/><br/>

                            Please compute those metrics in every 5 training iterations (epochs). Draw 2 plots for each model training: { epochs } vs. { training accuracy & testing accuracy } and { epochs } vs. { training loss & testing loss } [see <a href="https://i.stack.imgur.com/VIeBL.png" target="_blank">this example plots</a>].


                            <!-- :: Task 2 -->
                            <h5 style="margin-top: 20px;">Task II: Analyze the Impact of Your Training Techniques on Models</h5>

                            Now, let's turn our attention to how you train those 6 DNN models. You probably made various choices to train those models; for example, you may use cross-entropy to compute the loss of a model. Depending on how you train your models, they have slightly different properties. In this task, we will analyze the impact of various choices that you can make for training a model on its performance. Since this task may require training multiple DNN models, which takes some time, let's reduce our scope to two cases: (i) training LeNet on MNIST and (ii) ResNet18 on CIFAR10.<br/><br/>

                            You can control the following things:
                            <ul>
                                <li><b>Data augmentations:</b> transform the inputs of a neural network, e.g., cropping, resizing, flipping, shifting, ...</li>
                                <li><b>Model architectures:</b> add additional layers to a neural network, e.g., adding Dropout before the classification head, ...</li>
                                <li><b>Optimization algorithm (or loss functions):</b> choosing a different optimizer, e.g., SGD or Adam, or a different loss function.</li>
                                <li><b>Training hyper-parameters:</b> batch-size, learning rate, total number of training iterations (epochs), ...</li>
                            </ul>

                            Let's compare models trained in the following 5 scenarios:
                            <ul>
                                <li><b>Data augmentation: Rotation:</b> train your models with and w/o rotations and compare the plots.</li>
                                <li><b>Data augmentation: Horizontal flip:</b> train your models with and w/o random horizontal flips and compare the plots.</li>
                                <li><b>Optimization: SGD/Adam: </b> train your models with SGD or Adam and compare the plots.</li>
                                <li><b>Hyper-parameters: batch-size:</b> train your models with two different batch-sizes and compare the plots.</li>
                                <li><b>Hyper-parameters: learning rate:</b> train your models with two different learning rates and compare the plots.</li>
                            </ul>

                            You may (or may not) find a significant difference between the two models. Explain your intuitions on why you observe (or do not observe) them.

                            <!-- :: Submit -->
                            <h5 style="margin-top: 20px;">Submission Instructions</h5>

                            Use Canvas to submit your homework. You need to make a single compressed file (<code>.tar.gz</code>) that contains your code and a write-up as a PDF file. Put your write-up under the <code>reports</code> folder. Your PDF write-up should contain the following things:

                            <ul>
                                <li>Task I</li>
                                <ul>
                                    <li>Your experimental setup: specify your training configurations such as your hyper-parameter choices.</li>
                                    <li>Your 12 plots: 2 plots for each model, and you have 6 models.</li>
                                    <li>Your analysis: write-down a summary (the acc. and loss of the models); provide 2-3 sentences explaining why you see the results.</li>
                                </ul>

                                <li>Task II</li>
                                <ul>
                                    <li>Your 20 plots: 2 plots for each model, and you have 2 models for each of the five scenarios.</li>
                                    <li>Your analysis: Provide 2-3 sentences for each scenarios explaining why you observe the result.</li>
                                </ul>
                            </ul>

                        </div>
                    </div>
                </div>


                <!-- HW2 -->
                <div class="card">
                    <div class="card-header" id="headingTwo">
                        <button class="btn btn-link collapsed" 
                            type="button" 
                            style="color: black;"
                            data-toggle="collapse" 
                            data-target="#homework2" 
                            aria-expanded="true" 
                            aria-controls="homework2">
                            <h4>Homework 2: Adversarial Attacks on Your Models</h4>
                        </button>
                    </div>
              
                    <div id="homework2" 
                         class="collapse show" 
                         aria-labelledby="headingTwo"
                         data-parent="#homeworkExample">
                        <div class="card-body">

                            <!-- :: Deadlines -->
                            <h5>Important Dates</h5>
                            <ul>
                                <li><b style="color: #D73F09;">Check out the syllabus</b></li>
                            </ul>

                            <!-- :: Introduction -->
                            <h5 style="margin-top: 20px;">Homework Overview</h5>
                            <p>                                
                                The learning objective of this homework is for you to attack your models built in Homework 1 with white-box adversarial examples. You will also use adversarial training to build your robust models. We then analyze the impact of several factors—that you can control as an attacker or a defender—on the success rate of attack (or defense). You can start this homework from the codebase you wrote for Homework 1.
                            </p>


                            <!-- :: Setup -->
                            <h5 style="margin-top: 20px;">Initial Setup</h5>
                            <p>
                                <h6>Datasets and DNN Models</h6>

                                We will keep using the two datasets: MNIST [<a href="https://pytorch.org/vision/stable/datasets.html#mnist" target="_blank">link</a>] and CIFAR-10 [<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">link</a>]. But, we only focus on two DNN models: LeNet [<a href="https://en.wikipedia.org/wiki/LeNet" target="_blank">link</a>] and ResNet18 [<a href="https://arxiv.org/abs/1512.03385" target="_blank">link</a>].<br/><br/>
                                
                                <h6>Recommended Code Structure</h6>

                                You will write two scripts <code>adv_attack.py</code> and <code>adv_train.py</code>. The rest are the same as Homework 1.<br/>

                                <code>
                                    Root<br/>
                                    - [New] adv_attack.py: a Python script to run adversarial attacks on a pre-trained model.<br/>
                                    - [New] adv_train.py: a Python script for adversarial-training a model.<br/>
                                    ...<br/><br/>
                                </code>

                                <h6 style="color: #D73F09">Note</h6>
                                You may find off-the-shelf libraries, <i>e.g.</i>, adversarial-robustness-toolbox [<a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" target="_blank">link</a>], where you can plug-n-play attacks on your models. I do <b>NOT</b> recommend using any of those libraries for this homework. However, it is allowed to refer to the community implementations of attacks and defenses and re-write them in your hands. Remember: the important learning objective is to understand the attack internals and implement them. 
                            </p>


                            <!-- :: Task 1 -->
                            <h5 style="margin-top: 20px;">Task I: Attack Your Models</h5>
                            
                            Let's start with attacking your DNN models trained in Homework 1. We will attack your 2 DNNs: LeNet on MNIST and ResNet18 on CIFAR10. You need to use PGD [<a href="https://arxiv.org/abs/1706.06083" target="_blank">Madry et al.</a>] as an adversarial example-crafting algorithm. Your job is to craft the PGD adversarial examples for all the test-time samples (<i>i.e.</i>, 10k test-set samples for both MNIST and CIFAR10). To measure the effectiveness of your attacks, we will compute the <i>classification accuracy</i> on these adversarial examples. Make sure you attack the same DNNs that you used for crafting adversarial examples.<br/><br/>
                                
                            Here, you need to implement the following function in <code>adv_attack.py</code>.<br/>

                            <code>
                                def PGD(x, y, model, loss, niter, epsilon, stepsize, randinit, ...)<br/>
                                - x: a clean sample<br/>
                                - y: the label of x<br/>
                                - model: a pre-trained DNN you're attacking<br/>
                                - loss: a loss you will use<br/>
                                - [PGD params.] niter: # of iterations<br/>
                                - [PGD params.] epsilon: l-inf epsilon bound<br/>
                                - [PGD params.] stepsize: the step-size for PGD<br/>
                                - [PGD params.] randinit: start from a random perturbation if set true<br/>
                                // You can add more arguments if required<br/><br/>
                            </code>

                            This PGD function crafts the adversarial example for a sample (x, y) [or a batch of samples]. It takes (x, y), a pre-trained DNN, and attack parameters; and returns the adversarial example(s) (x', y). Note that you can add more arguments to this function if required. Please use the following attack hyper-parameters as a default:<br/>
                            <ul>
                                <li>niter: 5</li>
                                <li>epsilon: 0.3 (MNIST) and 0.03 (CIFAR10)</li>
                                <li>stepsize: 2/255.</li>
                                <li>randinit: true</li>
                            </ul>

                            To measure the effectiveness of the adversarial examples, we will write an evaluation script in <code>if __name__ == "__main__":</code> in the same file. Here, for all the 10k adversarial examples crafted, you will compute the classification accuracy on the DNN model you used. Note that you will observe much less accuracy than what you can observe on the <i>clean</i> test-time samples.<br/>


                            <!-- :: Task 2 -->
                            <h5 style="margin-top: 20px;">Task II: Analyze the Impact of Several Factors on Your Attack's Success Rate</h5>

                            Now, let's turn our attention to several factors that can increase/decrease the effectiveness of your white-box attacks. In particular, we will vary: (1) the attack hyper-parameters (<i>e.g.</i>, the number of iterations) and (2) the way we trained our DNN models (see Task II of Homework 1).<br/><br/>


                            <h6>Subtask II-1: Analyze the Impact of Attack Hyper-parameters</h6>

                            We will focus on two attack hyper-parameters: niter and epsilon. Use the 2 DNNs in Task I (LeNet on MNIST and ResNet18 on CIFAR10).<br/><br/>
                            <ul>
                                <li>
                                    (1) Set the number of iterations in {1, 2, 3, 4, 5, 10, 20, 30, 40, 80, 100}.
                                </li>
                                <li>
                                    (2) Fix the iterations to 5, and set the epsilon to {0.01 0.02 0.03 0.04 0.05 0.1 0.2 0.3 0.4 0.5 1.0}.
                                </li>
                            </ul>
                            
                            Please use those different hyper-parameters and compute the classification accuracy of 2 DNN models on your adversarial examples. Draw plots: { # iterations } vs. { classification accuracy } and { epsilon } vs. { classification accuracy } and explain your intuitions on why you observe them.<br/><br/>


                            <h6>Subtask II-2: Analyze the Impact of the Training Techniques You Use</h6>

                            One may think we can use some <i>nice</i> training techniques for reducing the effectiveness of white-box adversarial attacks. We plan to run some experiments to evaluate this claim. In particular, we're interested in the following three techniques: data augmentations and regularizations.<br/><br/>

                            <ul>
                                <li>
                                    (1) <b>Data augmentations:</b> In Task II of Homework 1, we examine two simple augmentations: rotation and horizontal flips. We also have DNNs trained with/without those augmentations. On the 2 DNNs (LeNet on MNIST and ResNet18 on CIFAR10) trained with/without each data augmentation, craft adversarial examples on the test-set samples and measure the classification accuracy on them.<br/><br/>
                                </li>
                                <li>
                                    (2) <b>Regularizations:</b> We also examine two techniques: Dropout [<a href="https://jmlr.org/papers/v15/srivastava14a.html" target="_blank">link</a>] and weight decay. Let's focus only on ResNet18 in CIFAR10.
                                    <ul>
                                        <li>1) To examine the impact of Dropout, we need to modify the ResNet18's network architecture. Add the Dropout layer before its penultimate layer and set the rate to 0.5. Train this modified ResNet18 (henceforth called ResNet18-Dropout). Craft adversarial examples on this model, measure the classification accuracy, and compare the accuracy to what we have with ResNet18 (w/o Dropout).</li>

                                        <li>2) To examine the impact of weight decay, we will train ResNet18 with Adam optimizer [<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" target="_blank">link</a>] on CIFAR10. You will train 5 ResNet18 models trained with different weight decay values: {1e-5, 1e-4, 1e-3, 1e-2, 1e-1}. Please don't be surprised when you see bad accuracy with higher weight decay. Craft adversarial examples on those five DNN models and measure the accuracy on both the clean samples and adversarial examples. Compare <i>how much accuracy you can decrease on each model.</i></li>
                                    </ul>
                                </li>
                            </ul>

                            You may (or may not) find that each technique increases/decreases the accuracy degradation caused by adversarial examples. Please write down the accuracy degradations and explain your intuitions on why you observe them in your report.


                            <!-- :: Task 3 -->
                            <h5 style="margin-top: 20px;">Task III: Defend Your Models with Adversarial Training</h5>

                            One way to mitigate adversarial attacks is to train your models with adversarial training (AT). Here, we will examine the effectiveness of AT.<br/><br/>

                            Let's implement a script for AT. Make a copy of your <code>train.py</code> and name it <code>adv_train.py</code>. We will convert the normal training process into <i>adversarial</i> training. In <code>train.py</code>, we train a model on a batch of clean training samples (in each batch). Instead, you need to make adversarial examples on the batch of clean samples and train your models on them. Note that this is slightly different from <a href="https://arxiv.org/abs/1412.6572" target="_blank">the work by Goodfellow et al.</a>.<br/><br/>

                            Please train 2 DNN models (LeNet and ResNet18) <i>adversarially</i> on MNIST and CIFAR10, respectively. Once you train those robust models, you require to craft adversarial examples and compute the accuracy. Note that we use the same attack hyperparameters as in Task I. Compare:<br/><br/>

                            <ul>
                                <li><b>(1)</b> How's your robust models' accuracy on adversarial examples compared to your undefended models?</li>
                                <li><b>(2)</b> How's your robust models' accuracy on clean test-set examples compared to your undefended models?</li>
                                <li><b>(3)</b> Let's increase the PGD attack iterations from 5 to 7. How's your robust models' accuracy changes?</li>
                            </ul>

                            Please explain your intuitions on why you observe them in your report.


                            <!-- :: Extra points -->
                            <h5 style="margin-top: 20px;"><span style="color: #D73F09">[Extra +3 pts]</span>: Use Your Adversarial Examples to Attack Real-world DNNs</h5>

                            You may be curious how much the adversarial examples that you crafted will be effective against the DNNs deployed in the real-world. Here are some real-world image classification demos [<a href="https://github.com/hwalsuklee/awesome-deep-vision-web-demo" target="_blank">a list of demos</a>]. Please store 10 adversarial examples for each MNIST and CIFAR10 attack (Task I) to <code>.png</code> files. Upload them on one of the image classification demos and see how the predicted labels are different compared to your DNNs.<br/><br/>

                            Please show your adversarial examples, the classification of them on your DNNs, and the predicted labels on the demo you chose in your report.

                            <!-- :: Submit -->
                            <h5 style="margin-top: 20px;">Submission Instructions</h5>

                            Use Canvas to submit your homework. You need to make a single compressed file (<code>.tar.gz</code>) that contains your code and a write-up as a PDF file. Put your write-up under the <code>reports</code> folder. Your PDF write-up should contain the following things:

                            <ul>
                                <li>Task I</li>
                                <ul>
                                    <li>The classification accuracy of clean test-set samples on 2 DNNs (LeNet and ResNet18).</li>
                                    <li>The classification accuracy of your adversarial examples on 2 DNNs.</li>
                                    <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                </ul>

                                <li>Task II</li>
                                <ul>
                                    <li>Subtask II-I</li>
                                    <ul>
                                        <li>Your 4 plots: { # iterations } vs. { classification accuacy } and { epsion } vs. { classification accuracy } on each of your 2 DNNs.</li>
                                        <li>Your analysis: Provide 2-3 sentences for each case explaining why you observe the result.</li>
                                    </ul>
                                    <li>Subtask II-II</li>
                                    <ul>
                                        <li>Your analysis: Provide 2-3 sentences for each case explaining why you observe the result.</li>
                                    </ul>
                                </ul>


                                <li>Task III</li>
                                <ul>
                                    <li>The classification accuracy of clean test-set samples on your robust DNNs.</li>
                                    <li>The classification accuracy of your adversarial examples on your robust DNNs.</li>
                                    <li>Your analysis: write-down 2-3 sentences for the three questions above.</li>
                                </ul>

                                <li><span style="color: #D73F09">[Extra +3 pts]</span></li>
                                <ul>
                                    <li>Your adversarial examples shown as images.</li>
                                    <li>Their classification results on your DNN models.</li>
                                    <li>Their classification results on the real-world DNNs.</li>
                                </ul>
                            </ul>

                        </div>
                    </div>
                </div>


                <!-- HW3 -->
                <div class="card">
                    <div class="card-header" id="headingTwo">
                        <button class="btn btn-link collapsed" 
                            type="button" 
                            style="color: black;"
                            data-toggle="collapse" 
                            data-target="#homework3" 
                            aria-expanded="true" 
                            aria-controls="homework3">
                            <h4>Homework 3: Data Poisoning Attacks and Defenses</h4>
                        </button>
                    </div>
              
                    <div id="homework3" 
                         class="collapse" 
                         aria-labelledby="headingTwo"
                         data-parent="#homeworkExample">
                        <div class="card-body">

                            <!-- :: Deadlines -->
                            <h5>Important Dates</h5>
                            <ul>
                                <li><b style="color: #D73F09;">Check out the syllabus</b></li>
                            </ul>

                            <!-- :: Introduction -->
                            <h5 style="margin-top: 20px;">Homework Overview</h5>
                            <p>                                
                                The learning objective of this homework is for you to perform data poisoning attacks on machine learning models (some of the attacks will require the neural networks trained in Homework 1). You will also test the effectiveness of simple defenses against the poisoning attacks you will implement. You can start this homework from the codebase you wrote in Homework 1.
                            </p>


                            <!-- :: Setup -->
                            <h5 style="margin-top: 20px;">Initial Setup</h5>
                            <p>
                                <h6>Datasets</h6>

                                We will use two datasets: MNIST-1/7 and CIFAR-10 [<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">link</a>]. MNIST-1/7 is a subset of MNIST that only contains samples from the class 1 and 7; the dataset is popular for a binary classification task. We can set the label for the class 1 to 0 and the class 7 to 1. Then, MNIST-1/7 becomes a binary classification problem with labels {0, 1}. Popular deep learning frameworks, such as PyTorch, support some sampling functionalities. Please search for some examples on Google about how to subsample classes from a dataset [Here is an <a href="https://stackoverflow.com/questions/63975130/how-to-get-only-specific-classes-from-pytorchs-fashionmnist-dataset" target="_blank">example</a> in PyTorch].<br/><br/>


                                <h6>Models</h6>

                                Here, we consider two models: logistic regression [<a href="https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19">an example</a> in PyTorch] for MNIST-1/7 and ResNet18 for CIFAR-10 [<a href="https://arxiv.org/abs/1512.03385">Link</a>].<br/><br/>


                                <h6>Recommended Code Structure</h6>

                                You will write three scripts <code>poison_craft.py</code>, <code>poisoning.py</code>, and <code>poison_remove.py</code>. The rest are the same as Homework 1.<br/>

                                <code>
                                    Root<br/>
                                    - [New] poison_craft.py&nbsp;: a Python script to craft poisoning samples.<br/>
                                    - [New] poison.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: a Python script for training a model on a contaminated dataset.<br/>
                                    - [New] poison_remove.py: a Python script for removing suspicious samples from a contaminated dataset.<br/>
                                    ...<br/><br/>
                                </code>
                            </p>


                            <!-- :: Task 1 -->
                            <h5 style="margin-top: 20px;">Task I: Poisoning Attack against Logistic Regression Models</h5>

                            Let's start with poisoning attacks against logistic regression models in MNIST-1/7. Here, we will conduct an <i>indiscriminate poisoning attack</i>. Your job is to construct contaminated training sets that can degrade the accuracy of a model once the model is trained on it.<br/><br/>

                            We will use a simple poisoning scheme called <i>random label-flipping</i>. It constructs a contaminated training set by randomly flipping the labels of <code>X%</code> samples in the original training set. For example, you can select 10% of the MNIST-1/7 training samples (~1.7k) and flip their labels from 0 to 1 (or vice versa).<br/><br/>

                            Your job is to construct four contaminated training sets where each set contains {5, 10, 25, 50}% of poisons. You will train five logistic regression models: four on each corrupted training set and one on the clean MNIST-1/7 dataset. Please measure how much accuracy degradation each attack causes compared to the accuracy of the model trained on the clean data.<br/><br/>


                            Here, you need to implement the following function in <code>poison_craft.py</code>.<br/><br/>

                            <code>
                                def craft_random_lflip(train_set, ratio):<br/>
                                - train_set: an instance for the training dataset<br/>
                                - ratio&nbsp;&nbsp;&nbsp;&nbsp;: the percentage of samples whose labels will be flipped<br/>
                                // You can add more arguments if required<br/><br/>
                            </code>

                            This function constructs a training set that has <code>ratio%</code> of poisons. The <code>train_set</code> is an instance of the clean training set and the <code>ratio</code> is a number between 0 and 1. Note that this is an example of writing a function for crafting poisoned training sets. Please feel free to use your own function if that is more convenient.<br/><br/>

                            You will also write the training script in <code>poison.py</code>. This script will be mostly the same as `train.py`, but the only difference is you load the contaminated training set instead of the clean training data. Once loaded, the rest will be the same.<br/><br/>


                            <!-- :: Task 2 -->
                            <h5 style="margin-top: 20px;">Task II: Poisoning Attacks on Deep Neural Networks</h5>

                            Now, let's turn our attention to attacking neural networks. As I explain in the lecture, deep neural networks are less susceptible to indiscriminate poisoning attacks, <i>i.e.</i>, it's hard to degrade their accuracy unless we inject many poisons. We therefore focus on <i>targeted poisoning attacks</i>.<br/><br/>

                            Your job here is to conduct Poison Frogs! [<a href="https://arxiv.org/abs/1804.00792" target="_blank">Link</a>] attack on ResNet18 trained on CIFAR-10. You can refer to the author's code [<a href="https://github.com/ashafahi/inceptionv3-transferLearn-poison" target="_blank">TensorFlow</a>] or the community implementations [<a href="https://github.com/FlouriteJ/PoisonFrogs" target="_blank">PyTorch</a>]. Be careful if you use those community code; there is a chance that they implement the attack incorrectly.<br/><br/>

                            <h6>Instructions</h6>

                            We conduct this attack between two classes in CIFAR-10: frogs and dogs. Particularly, we aim to make a frog sample classified into a dog. We will use the ResNet18 trained in Homework 1. Please follow the instructions below to inflict this misclassification.<br/><br/>

                            <ol>
                                <li>
                                    Choose 5 frog images (<b>targets</b>) from the CIFAR-10's test-set.
                                </li>
                                <li>
                                    Choose 100 dog images (<b>base images</b>) from the CIFAR-10's test-set. (You will use them to craft poisons).
                                </li>
                                <li>
                                    Use the 100 base images to craft <b>100 poisons</b> for each targets. Please use your ResNet18 to extract features. <span style="color: #D73F09">(see the details below)</span>.
                                </li>
                                <li>
                                    Construct <b>6 contaminated training sets</b> for each target by injecting {1, 5, 10, 25, 50, 100} poisons into the original training data.
                                </li>
                                <li>
                                    Finetune only the last layer of your ResNet18 for 10 epochs on each contaminated training set. Check if your finetuned model misclassifies each target (frog) as a dog. If the model misclassifies the target as a dog, your attack is successful. Otherwise, it's an attack failure.
                                </li>
                            </ol>

                            In total, you will have 30 contaminated training sets (= 6 different sets x 5 targets).<br/><br/>


                            <h6>Implementation</h6>

                            Here, you need to implement the following function in <code>poison_craft.py</code>.<br/><br/>

                            <code>
                                def craft_clabel_poisons(model, target, bases, niter, lr, beta, ...):<br/>
                                - model&nbsp;: a pre-trained ResNet18<br/>
                                - target: a target sample (a frog)<br/>
                                - bases&nbsp;: a set of base samples (dogs)<br/>
                                - niter&nbsp;: number of optimization iterations<br/>
                                - lr&nbsp;&nbsp;&nbsp;&nbsp;: learning rate for your optimization<br/>
                                - beta&nbsp;&nbsp;: hyper-parameter (refer to the paper)<br/>
                                // You can add more arguments if required<br/><br/>
                            </code>

                            This function crafts clean-label poisons. It takes a <code>model</code> (ResNet18) to extract features for a single target and 100 base samples. It also takes optimization hyper-parameters such as <code>niter</code>, <code>lr</code>, <code>beta</code>, etc. Once the function sufficiently optimizes your poisons, it will return 100 poisons crafted from the bases. Please refer to the author's code, the community implementations, and the original study for reference.<br/><br/>

                            You will also modify the training script in <code>poison.py</code>. This script will be mostly the same as <code>train.py</code>, but the only difference is you load the contaminated training set instead of the clean training data. Once loaded, the rest will be almost the same.<br/><br/>


                            <!-- :: Extra points -->
                            <h5 style="margin-top: 20px;"><span style="color: #D73F09">[Extra +3 pts]</span>: Defeat Data Poisoning Attacks</h5>

                            In the lecture, we learned two simple defense mechanisms against data poisoning attacks: (1) RONI [<a href="https://people.eecs.berkeley.edu/~tygar/papers/SML/Spam_filter.pdf" target="_blank">Paper</a>] and (2) Data sanitization [<a href="https://arxiv.org/abs/1706.03691" target="_blank">Link</a>]. Here, we implement those defenses and use them against the two data poisoning attacks (random label-flipping and clean-label poisoning). <br/></br>


                            <h6>Subtask I: RONI against Random Label-flipping</h6>

                            Let's start with RONI. You will choose the MNIST-1/7 training set containing 20% poisons (<i>i.e.</i> 20% samples have flipped labels).<br/><br/>

                            <ol>
                                <li>
                                    First, sub-sample 20% of <code>any</code> samples from the MNIST-1/7 training set. You will use this (<b>D_v</b>) to remove poisons from the training data.
                                </li>
                                <li>
                                    Next, let's split the contaminated training set. You can divide the contaminated training data (<b>D_tr</b>) into multiple sets (<b>D_tr_i</b>, where <b>i</b> in [1, 170]) where each set contains 100 training samples (c.f. this process will create approximately ~170 different sets).
                                </li>
                                <li>
                                    You first train a logistic regression on <b>D_tr_1</b>; compute the model's accuracy on <b>D_tr_1</b> and save it.
                                </li>
                                <li>
                                    Iteratively (from <b>i</b>=1, ..., 170), train your model on <b>D_tr_1 + ... + D_tr_i</b>. At each time, compare the i-th model's accuracy with the (i-1)-th model's. If the accuracy is reduced more than <code>X%</code> (a hyper-parameter of your choice), remove <b>D_tr_i</b> from the training set and continue.
                                </li>
                                <li>
                                    Use at least two <code>X%</code> values and check how many poisons you removed in each case. You also need to check how the accuracy of your model is after removing suspicious samples (<i>i.e.</i>, you will examine the effectiveness of RONI defense).
                                </li>
                            </ol>

                            <h6>Subtask II: Data Sanitization against Clean-label Poisoning</h6>

                            Let's move on and defeat clean-label poisoning (Poison Frogs!). Please choose <code>any</code> successful attack (<i>i.e.</i>, choose a target and 100 poisons).<br/><br/>

                            <ol>
                                <li>
                                    We will use ResNet18 fine-tuned on the contaminated training set. Let's first compute features for all the training samples with the model.
                                </li>
                                <li>
                                    Using the features (for 50k original training samples + 100 posions you add), we will detect suspicious samples and remove them from the training set. Please remove the outlier samples by running this UMAP example [<a href="https://umap-learn.readthedocs.io/en/latest/outliers.html" target="_blank">link</a>] on the collected features.
                                </li>
                                <li>
                                    Let's sanitize the training set by removing outliers. Please remove 2-3 amounts (100, 200, or 300) and compose sanitized training sets.
                                </li>
                                <li>
                                    Finetune your <i>original</i> ResNet18 on each sanitized training set and check whether the poisoning attack is successful.
                                </li>
                            </ol><br/>


                            <!-- :: Submit -->
                            <h5 style="margin-top: 20px;">Submission Instructions</h5>

                            Use Canvas to submit your homework. You need to make a single compressed file (<code>.tar.gz</code> or <code>.zip</code>) that contains your code and a write-up as a PDF file. <span style="color: #D73F09">Please do not include datasets and models in your submission.</span> Put your write-up under the <code>reports</code> folder. Your PDF write-up should contain the following things:<br/><br/>

                            <ul>
                                <li>Task I</li>
                                <ul>
                                    <li>Your plot: { the ratio of poisons in the training set } vs. { classification accuracy } on the test-set</li>
                                    <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                </ul>

                                <li>Task II</li>
                                <ul>
                                    <li>Your table: 2 rows (the upper one is for the number of poisons and the lower one is the number of successful attacks over 5 targets)</li>
                                    <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                </ul>

                                <li><span style="color: #D73F09">[Extra +3 pts]</span></li>
                                <ul>
                                    <li>Sub-task I</li>
                                    <ul>
                                        <li>Your plot: { # iterations } vs. { the accuracy of your model } on the test-set.</li>
                                        <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                    </ul>

                                    <li>Sub-task II</li>
                                    <ul>
                                        <li>Your analysis: write-down 2-3 sentences explaining whether you successfully mitigate clean-label poisoning or not. (If possible) analyze whether you can defeat more successfully when removing more suspicious samples.</li>
                                    </ul>
                                </ul>
                            </ul>

                        </div>
                    </div>
                </div>

                <!-- HW4: Final HW -->
                <div class="card">
                    <div class="card-header" id="headingTwo">
                        <button class="btn btn-link collapsed" 
                            type="button" 
                            style="color: black;"
                            data-toggle="collapse" 
                            data-target="#homework4" 
                            aria-expanded="true" 
                            aria-controls="homework4">
                            <h4>Homework 4: Membership Inference and Differential Privacy</h4>
                        </button>
                    </div>
              
                    <div id="homework4" 
                         class="collapse" 
                         aria-labelledby="headingTwo"
                         data-parent="#homeworkExample">
                        <div class="card-body">

                            <!-- :: Deadlines -->
                            <h5>Important Dates</h5>
                            <ul>
                                <li><b style="color: #D73F09;">Check out the syllabus</b></li>
                            </ul>

                            <!-- :: Introduction -->
                            <h5 style="margin-top: 20px;">Homework Overview</h5>
                            <p>                   
                                The learning objective of this homework is for you to understand (1) a mechanism for measuring the privacy leakage of machine learning (ML) models and (2) a mechanism to bound the leakage while training ML models. The best way to understand those mechanisms is to implement them by your hands. Here, we will focus on membership inference attacks, especially the one proposed by Yeom <i>et al.</i>, and the de-facto standard defense, differential privacy (DP). You can start this final homework from the codebase you used for HW 1-3, as usual.
                            </p>


                            <!-- :: Setup -->
                            <h5 style="margin-top: 20px;">Initial Setup</h5>
                            <p>
                                <h6>Datasets and DNN Models</h6>

                                
                                We will use two datasets: FashionMNIST [<a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html" target="_blank">link</a>], and CIFAR-10 [<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">link</a>]. Note that we swith from MNIST to FashionMNIST as Yeom <i>et al</i>'s will be less effective in MNIST. We consider two DNNs: VGG16 [<a href="https://arxiv.org/abs/1409.1556" target="_blank">link</a>] and ResNet18 [<a href="https://arxiv.org/abs/1512.03385" target="_blank">link</a>]. You will train VGG16 on FashionMNIST and ResNet18 on CIFAR-10.<br/><br/>

                                <h6>Recommended Code Structure</h6>

                                You will write two scripts <code>mi_attack.py</code> and <code>dp_train.py</code>. The rest are the same as HW 1-3.<br/>

                                <code>
                                    Root<br/>
                                    - [New] mi_attack.py: a Python script to run membership inference attacks (Yeom et al's).<br/>
                                    - [New] dp_train.py&nbsp;: a Python script for training a model with differential privacy.<br/>
                                    ...<br/><br/>
                                </code>
                            </p>


                            <!-- :: Task 1 -->
                            <h5 style="margin-top: 20px;">Task I: Membership Inference Attacks on Machine Learning Models</h5>

                            Let's start with doing membership inference attacks formulated by Yeom <i>et al.</i> [<a href="https://arxiv.org/abs/1709.01604" target="_blank">link</a>] on your DNN models. Your job is to implement this attack and evaluate its effectiveness. Please write your attack code to <code>mi_attack.py</code>.<br/><br/>


                            <b>Build Models:</b> You will first train the victim models. Please train a network on each dataset for at least 80 epochs. During training, you have two sub-tasks: (1) store the model at {10, 20, 30, 40, 50, 60, 70, 80} epochs and (2) draw a plot { # epochs } vs. { the train and test losses }. In the end, we have 16 models (8 in FashionMNIST and 8 in CIFAR-10) and two plots where each describes the training and testing losses over epochs.<br/><br/>

                            <span style="color: #D73F09">[Note]</span> 
                            
                            Please ensure you observe the max. accuracy of VGG16 and ResNet18 models <b>> 80%</b> on both datasets during training.<br/><br/>

                            <b>Prepare Datasets</b> for evaluating membership inference attacks. Your job is to construct a dataset to evaluate the attack's effectiveness. Typically, we choose a dataset with 10k samples, 5k chosen randomly from the training set (members) and the other 5k selected randomly from the testing set (non-members). You will compose two datasets from FashionMNIST and CIFAR10, respectively.<br/><br/>

                            <b>Perform Yeom <i>et al.</i> Attack:</b> We will do Yeom <i>et al.</i> membership inference attack on the 16 models we train.<br/><br/>
                            
                            The first step is to implement the attack. It uses a threshold (loss value) to identify whether a sample that the adversary queries is a member (or a non-member). If the victim model's loss on a sample is lower than the threshold, the sample is a member. Otherwise, it's a non-member. You can compute this loss threshold with the 5k members in your attack validation dataset.<br/><br/>

                            Suppose we have a threshold, we will use it to classify whether each of the 10k validation samples is a member (or a non-member). We will then compute the membership advantage over the 10k samples. Your job is to run this attack process on all the 16 models we trained.<br/><br/>

                            The final step is to create two plots using the collected membership advantages; each plot corresponds to FashionMNIST or CIFAR-10 results, respectively. In each plot, you will show { # epochs } vs. { membership advantage }.<br/><br/>


                            <!-- :: Task 2 -->
                            <h5 style="margin-top: 20px;">Task II: Differential Privacy as a Defense against Membership Inference</h5>

                            Let's turn our attention to defenses. We will consider the standard defense, <i>differentially-private stochastic gradient descent (DP-SGD)</i>, to defend against the Yeom <i>et al.</i> attacks [<a href="https://arxiv.org/abs/1607.00133" target="_blank">link</a>]. Your job is to train models with DP-SGD and compare the attack success with the above results (no DP-SGD).<br/><br/>

                            <b>Train Models with DP-SGD:</b> Thanks to the community's effort, we don't need to implement this mechanism from scratch. You can use the off-the-shelf libraries, such as Opacus [<a href="https://opacus.ai/" target="_blank">link</a>] in PyTorch or TF-Privacy [<a href="https://github.com/tensorflow/privacy" target="_blank">link</a>] in TensorFlow. You can incorporate DP-SGD into your current training script.<br/><br/>

                            You first copy your <code>train.py</code> to <code>dp_train.py</code> and modify the <code>dp_train.py</code> accordingly. Those examples [<a href="https://opacus.ai/tutorials/building_image_classifier" target="_blank">example1</a>, <a href="https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial.py" target="_blank">example2</a>] will guide you.<br/><br/>

                            Once the training script is written and running, we will then train our models with different privacy guarantees. Note that in DP-SGD, we control the hyper-parameter (epsilon) to bound a model's privacy leakage. We train 5 different models with epsilons in {1, 2, 4, 8, 10}. In total, we will have 10 models, 5 for FashionMNIST and 5 for CIFAR-10. Please train each model for at least 80 epochs and save the model with the best test acc.<br/><br/>

                            <span style="color: #D73F09">[Note]</span> Set all the training hyper-parameters to the same as those we use in Task I, except the learning rate. I recommend doubling the learning rate.<br/><br/>

                            <b>Run Yeom <i>et al.</i> on Models Trained with DP-SGD:</b> We now run the attack formulated by Yeom <i>et al.</i> on the 10 models trained with DP-SGD. Compute the membership advantages.<br/><br/>

                            The last step is to create two plots using the collected membership advantages; each plot corresponds to FashionMNIST or CIFAR-10, respectively. In each plot, you will show { epsilon } vs. { membership advantage } and { epsilon } vs. { model's accuracy }.<br/><br/>


                            <!-- :: Submit -->
                            <h5 style="margin-top: 20px;">Submission Instructions</h5>

                            Use Canvas to submit your homework. You need to make a single compressed file (<code>.tar.gz</code> or <code>.zip</code>) that contains your code and a write-up as a PDF file. <span style="color: #D73F09">Please do not include datasets and models in your submission.</span> Put your write-up under the <code>reports</code> folder. Your PDF write-up should contain the following things:<br/><br/>

                            <ul>
                                <li>Task I</li>
                                <ul>
                                    <li>Your two plots: { # epochs } vs. { train and test loss} for FashionMNIST and CIFAR-10.</li>
                                    <li>Your two plots: { # epochs } vs. { membership advantage } for FashionMNIST and CIFAR-10.</li>
                                    <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                </ul>

                                <li>Task II</li>
                                <ul>
                                    <li>Your two plots: { epsilon } vs. { test acc. and  membership advantage } for FashionMNIST and CIFAR-10.</li>
                                    <li>Your analysis: write-down 2-3 sentences explaining why you see those results.</li>
                                </ul>
                            </ul>

                            <!-- :: Thanks -->
                            <h5 style="margin-top: 20px;">Thanks for your hard-working!</h5>

                        </div>
                    </div>
                </div>

            </div>

            <hr noshade="" size="1">


            <!-- Footer -->
            <footer class="footer">
                <div class="container text-center">
                  <span class="text-muted">© Sanghyun Hong: 2022-Present.</span>
                </div>
            </footer>

        </div>

        <!-- Optional Javascript -->
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

    </body>
</html>
