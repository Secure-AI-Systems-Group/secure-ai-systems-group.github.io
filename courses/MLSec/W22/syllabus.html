<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Title -->
        <title>
            CS499/599 :: W22 :: Machine Learning Security
        </title>

        <!-- SEO -->
        <meta name="author" content="Sanghyun Hong">
        <meta name="description" content="Machine Learning Security Oregon State University">
        <meta name="keywords" content="sanghyun hong albert machine learning security oregon state university">

        <!-- Favicon -->
        <link rel="shortcut icon" href="/resources/favicon.ico" type="image/x-icon">

        <!-- Prevent js cache by Chrome -->
        <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
        <meta http-equiv="Pragma" content="no-cache" />
        <meta http-equiv="Expires" content="0" />

        <!-- Bootstrap start -->
        <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Arial:300,400,500,700" />
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

        <!-- CSS -->
        <link rel="stylesheet" type="text/css" href="/css/osu-fonts.css">
        <link rel="stylesheet" type="text/css" href="/css/osu-styles.css">

    </head>
    <body>

        <!-- Content -->
        <div class="container">

            <!-- Title -->
            <h2 class="title">
                <b>CS 499/599 :: Winter 2022 :: Machine Learning Security</b>
            </h2>

            <hr noshade="" size="1">

            <!-- Top navigation bar -->
            <ul class="nav">
                <li class="nav-item">
                    <a class="nav-link" href="index.html"><h5>Home</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" aria-current="page" href="syllabus.html"><h5>Syllabus</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link disabled" href="homework.html"><h5>Homework</h5></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link disabled" href="project.html"><h5>Project</h5></a>
                </li>
            </ul>

            <hr noshade="" size="1">


            <!-- Notice before the class -->
            <div class="container">
                <h4 style="color: #D73F09">Notice</h4>
                <p style="font: Stratum2; color: #D73F09">
                    This is a tentative outline. The course schedule, materials, homework, and grading scheme are subject to change before the first day of class.
                </p>
            </div>

            <hr noshade="" size="1">


            <!-- Textbook & Prerequisite -->
            <div class="container">
                <div class="row">
                    <div class="col-6">
                        <h4>Textbooks</h4>
                        <p>
                            <b>No required textbook.</b> Reading materials will be provided on the course website and/or distributed in class. If you lack the basics in machine learning (or deep learning), the following bibles can be helpful:
                        </p>
        
                        <ul>
                            <li>[FOD'20] Mathematics for Machine Learning <a href="https://mml-book.com/" target="_blank">[Link]</a> </li>
                            <li>[<span class="tab"></span>B'06] Pattern Recognition and Machine Learning <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">[Link]</a></li>
                            <li>[GBC'16] Deep Learning <a href="http://www.deeplearningbook.org/" target="_blank">[Link]</a></li>
                        </ul>
                        <br/>
        
                        <h4>Prerequisites</h4>
                        <p>
                            This course requires a basic understanding of machine learning. Please consider taking <i>CS 434 :: Machine Learning and Data Mining</i> first.
                        </p>
                    </div>
        
                    <div class="col-6">
                        <h4>Grading</h4>
                        <p>
                            Your final grade for this course will be based on the following scheme:
                        </p>
                        <ul>
                            <li><b>10%</b> Paper critiques and in-class presentation.</li>
                                <ul>
                                    <li><b>5%</b> Paper reviews / <b>5%</b> Paper presentation.</li>
                                </ul>
                            <li><b>35%</b> Homeworks (HW).</li>
                                <ul>
                                    <li><b>5%</b> HW 1 / <b>10%</b> HW 2 / <b>10%</b> HW 3 / <b>10%</b> HW 4.</li>
                                </ul>
                            <li><b>35%</b> Group project.</li>
                                <ul>
                                    <li><b>5%</b> Project proposal.</li>
                                    <li><b>5%</b> Checkpoint 1 / <b>5%</b> Checkpoint 2.</li>
                                    <li><b>10%</b> Final presentation.</li>
                                    <li><b>5%</b> Presentation reviews.</li>
                                </ul>

                            <li><b>20%</b> Final exam.</li>
                            <li><b style="color: #D73F09">[Bonus] ~40%</b> Extra point opportunities.</li>
                                <ul>
                                    <li><b style="color: #D73F09">5%</b> Scribe lecture notes (max. twice).</li>
                                    <li><b style="color: #D73F09">5%</b> Paper presentation (max. twice).</li>
                                    <li><b style="color: #D73F09">10%</b> Outstanding project work.</li>
                                    <li><b style="color: #D73F09">10%</b> Submitting the final project report to workshops. </li>
                                </ul>
                        </ul>
                    </div>
                </div>
            </div>

            <hr noshade="" size="1">


            <!-- Schedule -->
            <div class="container">
                <h4>Schedule</h4>
                <table class="table">

                    <!-- Header -->
                    <thead><tr style="background-color: #D73F09">
                        <th scope="col" style="text-align:center; color: white"><b>Date</b></th>
                        <th scope="col" style="text-align:center; color: white"><b>Topics</b></th>
                        <th scope="col" style="text-align:center; color: white"><b>Notes</b></th>
                        <th scope="col" style="text-align:center; color: white"><b>Readings</b></th>
                    </tr></thead>


                    <!-- Body of the course schedule -->
                    <tbody>
                    <tr><td colspan=4 style="text-align:center"><b>Part I: Overview and Motivation</b></td></tr>
                    <tr>
                        <td><b>Wed.<br/>01/05</b></td>
                        <td>Introduction<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://people.eecs.berkeley.edu/~adj/publications/paper-files/SecML-MLJ2010.pdf" target="_blank">The Security of Machine Learning</a><br/>
                        </td>
                    </tr>


                    <!-- Adversarial examples... -->
                    <tr><td colspan=4 style="text-align:center"><b>Part II: Adversarial Examples</b></td></tr>
                    <tr>
                        <td><b>Mon.<br/>01/10</b></td>
                        <td>Preliminaries<br/>[Slides]</td>
                        <td><b style="color: #000000">[HW 1 Due]</b></td>
                        <td>
                            <a href="https://arxiv.org/abs/1708.06131" target="_blank">Evasion Attacks against Machine Learning at Test Time</a><br/>
                            <a href="https://arxiv.org/abs/1312.6199" target="_blank">Intriguing Properties of Neural Networks</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>01/12</b></td>
                        <td>Preliminaries<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1412.6572" target="_blank">Explaining and Harnessing Adversarial Examples</a><br/>
                            <a href="https://arxiv.org/abs/1607.02533" target="_blank">Adversarial Examples in the Physical World</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Mon.<br/>01/17</b></td>
                        <td>Attacks<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1608.04644" target="_blank">Towards Evaluating the Robustness of Neural Networks</a><br/>
                            <a href="https://arxiv.org/abs/1706.06083" target="_blank">Towards Deep Learning Models Resistant to Adversarial Attacks</a><br/>
                            [Bonus] <a href="https://arxiv.org/abs/1610.08401" target="_blank">Universal Adversarial Perturbations</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>01/20</b></td>
                        <td>Attacks<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1611.02770" target="_blank">Delving into Transferable Adversarial Examples and Black-box Attacks</a><br/>
                            <a href="https://arxiv.org/abs/1807.07978" target="_blank">Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors</a><br/>
                            [Bonus] <a href="https://arxiv.org/abs/1704.03453" target="_blank">The Space of Transferable Adversarial Examples</a><br/>
                        </td>
                    </tr>

                    <!-- :: MLK :: -->
                    <tr style="background-color: #B8DDE1">
                        <td><b>Mon.<br/>01/24</b></td>
                        <td><b>Martin Luther King Jr. Day</b></td>
                        <td><b>[No lecture]</b></td>
                        <td></td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>01/26</b></td>
                        <td>Defenses<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1704.01155" target="_blank">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</a><br/>
                            <b>[Revisit'ed]</b> <a href="https://arxiv.org/abs/1706.06083" target="_blank">Towards Deep Learning Models Resistant to Adversarial Attacks</a><br/>
                        </td>
                    </tr>

                    <!-- :: Group Projects :: -->
                    <tr style="background-color: #F38F50">
                        <td><b>Mon.<br/>01/31</b></td>
                        <td><b>Group Project</b></td>
                        <td></td>
                        <td><b>Checkpoint 1</b></td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>02/02</b></td>
                        <td>(Certified) Defenses<br/>[Slides]</td>
                        <td><b style="color: #000000">[HW 2 Due]</b></td>
                        <td>
                            <a href="https://arxiv.org/abs/1802.03471" target="_blank">Certified Robustness to Adversarial Examples with Differential Privacy</a><br/>
                            <a href="https://arxiv.org/abs/2003.01908" target="_blank">Denoised Smoothing: A Provable Defense for Pretrained Classifiers</a><br/>
                            [Bonus] <a href="https://arxiv.org/abs/1902.02918" target="_blank">Certified Adversarial Robustness via Randomized Smoothing</a><br/>
                        </td>
                    </tr>


                    <!-- Data poisonig attacks... -->
                    <tr><td colspan=4 style="text-align:center"><b>Part III: Data Poisoning</b></td></tr>
                    <tr>
                        <td><b>Mon.<br/>02/07</b></td>
                        <td>Preliminaries<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://people.eecs.berkeley.edu/~tygar/papers/SML/Spam_filter.pdf" target="_blank">Exploiting Machine Learning to Subvert Your Spam Filter</a><br/>
                            <a href="https://personal.utdallas.edu/~muratk/courses/dmsec_files/rpca_imc09.pdf" target="_blank">
                                ANTIDOTE: Understanding and Defending against Poisoning of Anomaly Detectors</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>02/09</b></td>
                        <td>Attacks<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1804.00308" target="_blank">
                                Manipulating Machine Learning:<br/>
                                &nbsp;&nbsp;&nbsp;&nbsp;Poisoning Attacks and Countermeasures for Regression Learning</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Mon.<br/>02/16</b></td>
                        <td>Attacks<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1804.00792" target="_blank">Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks</a><br/>
                            <a href="https://arxiv.org/abs/2004.00225" target="_blank">MetaPoison: Practical General-purpose Clean-label Data Poisoning</a><br/>
                            [Bonus] <a href="http://proceedings.mlr.press/v97/zhu19a.html" target="_blank">Transferable Clean-Label Poisoning Attacks on Deep Neural Nets</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>02/24</b></td>
                        <td>Defenses<br/>[Slides]</td>
                        <td><b style="color: #000000">[HW 3 Due]</b></td>
                        <td>
                            <a href="https://arxiv.org/abs/1706.03691" target="_blank">Certified Defenses for Data Poisoning Attacks</a><br/>
                            <a href="https://arxiv.org/abs/1803.02815" target="_blank">SEVER: A Robust Meta-Algorithm for Stochastic Optimization</a><br/>
                        </td>
                    </tr>

                    <tr style="background-color: #F38F50">
                        <td><b>Mon.<br/>02/21</b></td>
                        <td><b>Group Project</b></td>
                        <td></td>
                        <td><b>Checkpoint 2</b></td>
                    </tr>


                    <!-- Privacy attacks (MI)... -->
                    <tr><td colspan=4 style="text-align:center"><b>Part IV: Privacy</b></td></tr>
                    <tr>
                        <td><b>Wed.<br/>02/23</b></td>
                        <td>Preliminaries<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://privacytools.seas.harvard.edu/files/privacytools/files/pdf_02.pdf" target="_blank">Exposed! A Survey of Attacks on Private Data</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Mon.<br/>02/28</b></td>
                        <td>Attack<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1610.05820" target="_blank">Membership Inference Attacks against Machine Learning Models</a><br/>
                            <a href="https://arxiv.org/abs/1709.01604" target="_blank">Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting</a><br/>
                        </td>
                    </tr>
                    
                    <tr>
                        <td><b>Wed.<br/>03/02</b></td>
                        <td>Attack<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://dl.acm.org/doi/10.1145/2810103.2813677" target="_blank">Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures</a><br/>
                            <a href="https://arxiv.org/abs/1802.08232" target="_blank">The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks</a>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Mon.<br/>03/07</b></td>
                        <td>Attack<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1609.02943" target="_blank">Stealing Machine Learning Models via Prediction APIs</a><br/>
                        </td>
                    </tr>

                    <tr>
                        <td><b>Wed.<br/>03/09</b></td>
                        <td>(Certified) Defense<br/>[Slides]</td>
                        <td></td>
                        <td>
                            <a href="https://arxiv.org/abs/1607.00133" target="_blank">Deep Learning with Differential Privacy</a><br/>
                            <a href="https://arxiv.org/abs/1902.08874" target="_blank">Evaluating Differentially Private Machine Learning in Practice</a><br/>
                            [Bonus] <a href="https://arxiv.org/abs/2101.04535" target="_blank">Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning</a><br/>
                        </td>
                    </tr>

                    <tr style="background-color: #F38F50">
                        <td><b>Mon.<br/>03/14</b></td>
                        <td><b>Group Project</b></td>
                        <td><b style="color: #000000">[HW 4 Due]</b></td>
                        <td><b>Final Presentations (Showcases)</b></td>
                    </tr>

                    <!-- Finals -->
                    <tr style="background-color: #B8DDE1">
                        <td><b>Wed.<br/>03/16</b></td>
                        <td><b>Last Day of Classes</b></td>
                        <td><b>Final Exam<br/>[No Class]</b></td>
                        <td></td>
                    </tr>

                </table>
            </div>


            <!-- Footer -->
            <hr noshade="" size="1">
            <footer class="footer">
                <div class="container text-center">
                  <span class="text-muted">© Sanghyun Hong: 2022-Present.</span>
                </div>
            </footer>

        </div>

        <!-- Optional Javascript -->
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

    </body>
</html>